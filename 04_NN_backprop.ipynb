{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_NN_backprop_solution(심화_추가).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUBTeOIwhlBj"
      },
      "source": [
        "\n",
        "## 1. 지난시간 복습\n",
        "\n",
        "Cross-entrophy loss는 다음과 같이 정의한다. \n",
        "\n",
        "$CELoss = -  \\sum^C_{i=1} (y_i \\cdot \\log \\hat{y_i})$\n",
        "\n",
        "\n",
        "여기서 C는 분류할 클래스의 개수이다. \n",
        "우리는 Loss를 모든 데이터에 기반하여 계산할 것이므로, 최종으로 구할 L는 모든 데이터 M개에 관한 CELoss의 평균을 이용한다. \n",
        "\n",
        "$L = - \\sum^{M}_{j=1} \\sum^C_{i=1} (y_i^j \\cdot \\log \\hat{y_i^j})$\n",
        "\n",
        "일반적으로 CELoss는 Softmax가 적용된 출력을 입력으로 받으며, softmax+CELoss를 입력값으로 미분하게 될 때 매우 간결한 형태의 analytic gradient를 얻는다. \n",
        "\n",
        "- $\\frac{\\partial L}{\\partial z_i} = p_i - y_i$\n",
        "\n",
        "\n",
        "- 여기서 $z_i$는 softmax에 들어가는 입력 (혹은 앞 레이어의 출력)\n",
        "\n",
        "\n",
        "\n",
        "CELoss + Softmax 결합 모듈의 미분 증명 \n",
        "\n",
        "- https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQpj4TuSvgmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cafb81-0991-44de-f8f0-0dc68ba3bd22"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# softmax function: np.exp에 너무 큰값이 들어가지 않도록 수정\n",
        "def Softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "# CELoss \n",
        "def CELoss(Y_hat, labels): \n",
        "    n_data = len(labels)\n",
        "    loss = 0 \n",
        "    for i in range(n_data):        \n",
        "        celoss = np.sum(- (labels[i]*np.log(Y_hat[i])))\n",
        "        loss += celoss/n_data\n",
        "    return loss\n",
        "    \n",
        "# 데이터\n",
        "x_1 = np.array([[56, 231],\n",
        "                   [24, 1]])\n",
        "x_2 = np.array([[120, 30],\n",
        "                   [24, 0]])\n",
        "x_3 = np.array([[20, 31],\n",
        "                   [240, 100]])\n",
        "x_4 = np.array([[56, 201],\n",
        "                   [22, -10]])\n",
        "x_5 = np.array([[140, 27],\n",
        "                   [30, 10]])\n",
        "x_6 = np.array([[25, 30],\n",
        "                   [230, 110]])\n",
        "\n",
        "\n",
        "# one-hot vector 정의하기\n",
        "y_1 = np.array([1,0,0])   #label_cat\n",
        "y_2 = np.array([0,1,0])   #label_dog\n",
        "y_3 = np.array([0,0,1])   #label_ship \n",
        "y_4 = np.array([1,0,0])   #label_cat_2\n",
        "y_5 = np.array([0,1,0])   #label_dog_2\n",
        "y_6 = np.array([0,0,1])   #label_ship_2 \n",
        "\n",
        "\n",
        "#  데이터셋\n",
        "X = np.array([x_1, x_2, x_3, x_4, x_5, x_6]) # data\n",
        "Y = np.array([y_1, y_2, y_3, y_4, y_5, y_6]) # label\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2, 2)\n",
            "(6, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y-7CwpdAEvL"
      },
      "source": [
        "## 실습과제 . 지난시간에 구현한 뉴럴넷 Class 형태로 재구현\n",
        "\n",
        "지난 시간에 구현하였던 간단한 선형분류기+Softmax+CELoss 구조를 갖는 뉴럴네트워크를 아래와 같이 python class로 구현하였다. \n",
        "\n",
        "- 구조: x -> Linear(4,3) -> Softmax -> CELoss <- y_hat\n",
        "\n",
        "여기서 Linear(in, out)은 입력in개와 출력 out개를 갖는 선형분류기를 뜻한다. Backward 함수를 구현하여 본 뉴럴넷의 학습을 동작하게 만드시오. 각 레이어에 대한 local gradient 계산법은 아래에 첨부하였다. \n",
        "\n",
        "------------------------------\n",
        "\n",
        "각 레이어에 대한 미분값\n",
        "\n",
        "Derivative of CELoss with softmax :\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial x_i} = p_i - y_i$\n",
        "\n",
        "- 여기서 $x_i$는 softmax에 들어가는 입력 (혹은 앞 레이어의 출력 $z_i$)\n",
        "\n",
        "- ($L = CELoss(softmax(x), y) = CELoss(p,y)$) \n",
        "\n",
        "- https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
        "\n",
        "Derivative of Linear layer (Y = WX):\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Y} X^T$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial X} = W^T \\frac{\\partial L}{\\partial Y} $\n",
        "\n",
        "\n",
        "- http://cs231n.stanford.edu/handouts/linear-backprop.pdf\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am8MRwZmyAPi",
        "outputId": "21d25a60-ca67-4ea0-f52d-cde95224c27c"
      },
      "source": [
        "# Single-layer Neural Network class\n",
        "\n",
        "class NN_1layer():\n",
        "    def __init__(self, **kwargs):\n",
        "        self.W_l1 = np.random.rand(3,4)\n",
        "        self.activation = Softmax\n",
        "        self.Loss = CELoss\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "    def Forward(self, X):\n",
        "        # initial prediction\n",
        "        Y_hat = []\n",
        "        \n",
        "        for x in X: \n",
        "            # layer 1\n",
        "            x = np.matmul(self.W_l1, x.flatten())\n",
        "          \n",
        "            # softmax\n",
        "            p = self.activation(x)\n",
        "\n",
        "            Y_hat.append(p)\n",
        "        return Y_hat\n",
        "        \n",
        "    def getLoss(self, X, Y):\n",
        "        # y_hat -> CELoss <- y\n",
        "        Y_hat = self.Forward(X)\n",
        "        return self.Loss(Y_hat, Y)\n",
        "\n",
        "    def Backward(self, Y_hat, X, Y): \n",
        "        dW1Loss = np.zeros_like(self.W_l1)\n",
        "        N = X.shape[0] # 데이터 개수\n",
        "\n",
        "        ####################################\n",
        "        ##### 이곳에 코드를 작성하시오 #####\n",
        "        # backpropagation 구현 힌트 \n",
        "        # 1. softmax와 CELoss는 한개의 레이어로 간주하고 미분\n",
        "        #   x -> [Linear(4,3)] -> [Softmax -> CELoss] <- y_hat\n",
        "        # 2. 두 레이어의 gradient 공식은 위의 각 레이어 미분 공식을 분석\n",
        "        #   분석 시 함수, 입력, 출력 기호를 명확하게 파악할 것!\n",
        "        \n",
        "        # Backpropagation 공식\n",
        "        #   Downstream gradient = UpstreamGradient * LocalGradient\n",
        "        # - 각 레이어 별 LocalGradient 계산법을 파악\n",
        "        # - 각 레이어 별 UpstreamGradient가 무엇인지 파악\n",
        "        # - 위 두가지를 곱하여 downstreamGradient를 계산\n",
        "      \n",
        "        ##### 참고 사항. np 행렬의 형상 차이에 주의 \n",
        "        # shape (N,)은 1차원 배열로써 행렬의 transpose가 적용되지 않으므로 \n",
        "        # reshape(N,1)로 차원을 명시한 뒤에 transpose를 적용하시오. \n",
        "        # 사용 예)  x.reshape(4,1).T\n",
        "\n",
        "        # 개별 데이터 단위 연산\n",
        "        for i, x in enumerate(X): \n",
        "            dCELoss = Y_hat[i] - Y[i]\n",
        "            dW1Loss += np.matmul(dCELoss.reshape(3,1), x.reshape(4,1).T)\n",
        "\n",
        "\n",
        "        # 데이터 행렬 \n",
        "\n",
        "        ####################################\n",
        "\n",
        "        return dW1Loss/N\n",
        "\n",
        "    def training(self, X, Y, n_epoch=100):\n",
        "        iterations = 0\n",
        "        while iterations < n_epoch: \n",
        "            # gradient descent algorithm \n",
        "            Y_hat = self.Forward(X)\n",
        "            dWLoss = self.Backward(Y_hat, X, Y)\n",
        "\n",
        "            self.W_l1 -= dWLoss*self.learning_rate\n",
        "            print(f'iteration: {iterations+1}, loss: {self.getLoss(X,Y)}')\n",
        "            iterations += 1\n",
        "\n",
        "my_NeuralNet = NN_1layer()\n",
        "my_NeuralNet.training(X, Y)\n",
        "\n",
        "# 학습이 잘되었다면 [1,0,0] 과 같은 형태로 출력\n",
        "print(my_NeuralNet.Forward(X))\n",
        "\n",
        "# 학습이 잘되었다면 매우 작은값(예: 0.01) 형태가 출력된다.\n",
        "print(my_NeuralNet.getLoss(X, Y))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1, loss: 53.32741088981144\n",
            "iteration: 2, loss: 51.27258995044743\n",
            "iteration: 3, loss: 49.21776945234153\n",
            "iteration: 4, loss: 47.16294934510085\n",
            "iteration: 5, loss: 45.10812958455894\n",
            "iteration: 6, loss: 43.05331013196445\n",
            "iteration: 7, loss: 40.99849095327901\n",
            "iteration: 8, loss: 38.94367201856942\n",
            "iteration: 9, loss: 36.88885330148117\n",
            "iteration: 10, loss: 34.83403477878206\n",
            "iteration: 11, loss: 32.77921642996658\n",
            "iteration: 12, loss: 30.724398236912602\n",
            "iteration: 13, loss: 28.66958018358326\n",
            "iteration: 14, loss: 26.614762255768092\n",
            "iteration: 15, loss: 24.55994444085801\n",
            "iteration: 16, loss: 22.50512672765023\n",
            "iteration: 17, loss: 20.450309106185454\n",
            "iteration: 18, loss: 18.395491567843223\n",
            "iteration: 19, loss: 16.34067411452207\n",
            "iteration: 20, loss: 14.28585712131781\n",
            "iteration: 21, loss: 12.231055512249021\n",
            "iteration: 22, loss: 10.17685274875087\n",
            "iteration: 23, loss: 8.145186969063381\n",
            "iteration: 24, loss: 6.5042429804285575\n",
            "iteration: 25, loss: 5.419243822360853\n",
            "iteration: 26, loss: 4.353832271510715\n",
            "iteration: 27, loss: 3.288974513107714\n",
            "iteration: 28, loss: 2.2304360195655466\n",
            "iteration: 29, loss: 1.296975567244331\n",
            "iteration: 30, loss: 0.8271444759613784\n",
            "iteration: 31, loss: 0.5645037202735483\n",
            "iteration: 32, loss: 0.35279490408242525\n",
            "iteration: 33, loss: 0.20081249543486268\n",
            "iteration: 34, loss: 0.1196739099122841\n",
            "iteration: 35, loss: 0.0815376827946548\n",
            "iteration: 36, loss: 0.061344410811947386\n",
            "iteration: 37, loss: 0.04907777018512357\n",
            "iteration: 38, loss: 0.04087278136659329\n",
            "iteration: 39, loss: 0.03500741916390526\n",
            "iteration: 40, loss: 0.030608880096368234\n",
            "iteration: 41, loss: 0.027189434370237246\n",
            "iteration: 42, loss: 0.024455616769327684\n",
            "iteration: 43, loss: 0.02222041086055253\n",
            "iteration: 44, loss: 0.020359054067527017\n",
            "iteration: 45, loss: 0.018785150539792128\n",
            "iteration: 46, loss: 0.017436989686417204\n",
            "iteration: 47, loss: 0.016269325212248244\n",
            "iteration: 48, loss: 0.015248232196820624\n",
            "iteration: 49, loss: 0.01434777738550976\n",
            "iteration: 50, loss: 0.01354779864085825\n",
            "iteration: 51, loss: 0.012832385380373398\n",
            "iteration: 52, loss: 0.012188814853057315\n",
            "iteration: 53, loss: 0.011606792388540647\n",
            "iteration: 54, loss: 0.011077898928271811\n",
            "iteration: 55, loss: 0.01059518275820421\n",
            "iteration: 56, loss: 0.010152853380160153\n",
            "iteration: 57, loss: 0.009746048915738154\n",
            "iteration: 58, loss: 0.009370657237706151\n",
            "iteration: 59, loss: 0.009023176892274082\n",
            "iteration: 60, loss: 0.008700607858223098\n",
            "iteration: 61, loss: 0.008400364935614477\n",
            "iteration: 62, loss: 0.008120208479497503\n",
            "iteration: 63, loss: 0.007858188558459623\n",
            "iteration: 64, loss: 0.0076125995984689535\n",
            "iteration: 65, loss: 0.007381943285541925\n",
            "iteration: 66, loss: 0.007164898025012986\n",
            "iteration: 67, loss: 0.0069602936445760915\n",
            "iteration: 68, loss: 0.006767090320249654\n",
            "iteration: 69, loss: 0.006584360925340156\n",
            "iteration: 70, loss: 0.006411276171040496\n",
            "iteration: 71, loss: 0.006247092036942904\n",
            "iteration: 72, loss: 0.006091139090188694\n",
            "iteration: 73, loss: 0.005942813370360014\n",
            "iteration: 74, loss: 0.005801568578793499\n",
            "iteration: 75, loss: 0.005666909359661683\n",
            "iteration: 76, loss: 0.005538385498883769\n",
            "iteration: 77, loss: 0.005415586897881857\n",
            "iteration: 78, loss: 0.005298139204098143\n",
            "iteration: 79, loss: 0.005185700000310024\n",
            "iteration: 80, loss: 0.005077955471132658\n",
            "iteration: 81, loss: 0.004974617478429503\n",
            "iteration: 82, loss: 0.004875420988305765\n",
            "iteration: 83, loss: 0.004780121801345039\n",
            "iteration: 84, loss: 0.004688494545211683\n",
            "iteration: 85, loss: 0.0046003308949102845\n",
            "iteration: 86, loss: 0.0045154379911466975\n",
            "iteration: 87, loss: 0.004433637031537718\n",
            "iteration: 88, loss: 0.004354762013030512\n",
            "iteration: 89, loss: 0.004278658606933067\n",
            "iteration: 90, loss: 0.004205183150526074\n",
            "iteration: 91, loss: 0.0041342017414021295\n",
            "iteration: 92, loss: 0.004065589422530715\n",
            "iteration: 93, loss: 0.003999229447619937\n",
            "iteration: 94, loss: 0.003935012617695115\n",
            "iteration: 95, loss: 0.0038728366809710063\n",
            "iteration: 96, loss: 0.003812605789081252\n",
            "iteration: 97, loss: 0.0037542300035889285\n",
            "iteration: 98, loss: 0.0036976248474363768\n",
            "iteration: 99, loss: 0.0036427108966305614\n",
            "iteration: 100, loss: 0.003589413408018342\n",
            "[array([9.99996281e-01, 5.44983576e-11, 3.71917447e-06]), array([9.71181862e-34, 9.99999950e-01, 4.97797203e-08]), array([3.00587341e-03, 1.26787132e-60, 9.96994127e-01]), array([9.87162987e-01, 7.59878882e-04, 1.20771338e-02]), array([1.33657710e-36, 9.99999305e-01, 6.94556282e-07]), array([5.58583848e-03, 9.27950449e-61, 9.94414162e-01])]\n",
            "0.003589413408018342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUD0Md6JLrBk"
      },
      "source": [
        "-----------------------------------------\n",
        "\n",
        "# 심화 연습문제 (optional)\n",
        "\n",
        "위 네트워크 구조를 아래와 같이 2 layer로 바꾸어 보고, 학습을 성공시켜 보시오. init(), Forward와 Backward 함수를 수정해야 한다. \n",
        "\n",
        "- 구조: x -> Linear(4,4) -> ReLU ->  Linear(4,3) -> Softmax -> CELoss <- y_hat\n",
        "\n",
        "ReLU는 아래의 구현을 참고하시오. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aahJfjXMMtx"
      },
      "source": [
        "# ReLU\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# derivatives of ReLU\n",
        "def dReLU(x):\n",
        "    # if (x > 0):\n",
        "    #     return 1\n",
        "    # if (x <= 0):\n",
        "    #     return 0\n",
        "    return 1 * (x > 0)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX6KovjDQJKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cfec2d3-23ae-4372-c2af-b7e9b90bd09a"
      },
      "source": [
        "# Two-layer Neural Network class\n",
        "import numpy as np\n",
        "\n",
        "class NN_2layer():\n",
        "    def __init__(self, **kwargs):\n",
        "        self.W_l1 = np.random.rand(4,4)\n",
        "        self.W_l2 = np.random.rand(3,4)\n",
        "        self.activation = Softmax\n",
        "        self.Loss = CELoss\n",
        "        self.learning_rate = 0.001\n",
        "        self.z_2 = 0\n",
        "\n",
        "    def Forward(self, X):\n",
        "        # initial prediction\n",
        "        Y_hat = []\n",
        "        \n",
        "        for x in X: \n",
        "            # layer 1\n",
        "            z_1 = np.matmul(self.W_l1, x.flatten())\n",
        "          \n",
        "            # layer 2\n",
        "            z_2 = ReLU(z_1)\n",
        "            self.z_2 = z_2\n",
        "\n",
        "            # layer 3\n",
        "            z_3 = np.matmul(self.W_l2, z_2)\n",
        "                        \n",
        "            # softmax\n",
        "            p = self.activation(z_3)\n",
        "\n",
        "            Y_hat.append(p)\n",
        "        return Y_hat\n",
        "        \n",
        "    def getLoss(self, X, Y):\n",
        "        # y_hat -> CELoss <- y\n",
        "        Y_hat = self.Forward(X)\n",
        "        return self.Loss(Y_hat, Y)\n",
        "\n",
        "    def Backward(self, Y_hat, X, Y): \n",
        "        dW1Loss = np.zeros_like(self.W_l1)\n",
        "        dW2Loss = np.zeros_like(self.W_l2)\n",
        "        N = X.shape[0] # 데이터 개수\n",
        "\n",
        "        ####################################\n",
        "        ##### 이곳에 코드를 작성하시오 #####\n",
        "        # backpropagation 구현 힌트 \n",
        "        # 1. softmax와 CELoss는 한개의 레이어로 간주하고 미분\n",
        "        #   x -> [Linear(4,3)] -> [Softmax -> CELoss] <- y_hat\n",
        "        # 2. 두 레이어의 gradient 공식은 위의 각 레이어 미분 공식을 분석\n",
        "        #   분석 시 함수, 입력, 출력 기호를 명확하게 파악할 것!\n",
        "        \n",
        "        # Backpropagation 공식\n",
        "        #   Downstream gradient = UpstreamGradient * LocalGradient\n",
        "        # - 각 레이어 별 LocalGradient 계산법을 파악\n",
        "        # - 각 레이어 별 UpstreamGradient가 무엇인지 파악\n",
        "        # - 위 두가지를 곱하여 downstreamGradient를 계산\n",
        "      \n",
        "        ##### 참고 사항. np 행렬의 형상 차이에 주의 \n",
        "        # shape (N,)은 1차원 배열로써 행렬의 transpose가 적용되지 않으므로 \n",
        "        # reshape(N,1)로 차원을 명시한 뒤에 transpose를 적용하시오. \n",
        "        # 사용 예)  x.reshape(4,1).T\n",
        "\n",
        "        for i, x in enumerate(X): \n",
        "            # CElayer Loss\n",
        "            dz3Loss = Y_hat[i] - Y[i]\n",
        "\n",
        "            # dW2Loss\n",
        "            dW2Loss += np.matmul(dz3Loss.reshape(3,1), self.z_2.reshape(4,1).T)\n",
        "\n",
        "            # dz2Loss\n",
        "            dz2Loss = np.matmul(self.W_l2.T , dz3Loss.reshape(3,1))\n",
        "\n",
        "            # dz2Loss\n",
        "            dz1Loss = dReLU(dz2Loss) * dz2Loss\n",
        "\n",
        "            # dW1Loss\n",
        "            dW1Loss += np.matmul(dz1Loss.reshape(4,1), x.reshape(4,1).T)\n",
        "\n",
        "        ####################################\n",
        "\n",
        "        return dW1Loss/N, dW2Loss/N\n",
        "\n",
        "    def training(self, X, Y, n_epoch=100):\n",
        "        iterations = 0\n",
        "        while iterations < n_epoch: \n",
        "            # gradient descent algorithm \n",
        "            Y_hat = self.Forward(X)\n",
        "            dW1Loss, dW2Loss = self.Backward(Y_hat, X, Y)\n",
        "\n",
        "            self.W_l1 -= dW1Loss*self.learning_rate\n",
        "            self.W_l2 -= dW2Loss*self.learning_rate\n",
        "            print(f'iteration: {iterations+1}, loss: {self.getLoss(X,Y)}')\n",
        "            iterations += 1\n",
        "\n",
        "my_NeuralNet = NN_2layer()\n",
        "my_NeuralNet.training(X, Y)\n",
        "\n",
        "# 학습이 잘되었다면 [1,0,0] 과 같은 형태로 출력\n",
        "print(my_NeuralNet.Forward(X))\n",
        "\n",
        "# 학습이 잘되었다면 매우 작은값(예: 0.01) 형태가 출력된다.\n",
        "print(my_NeuralNet.getLoss(X, Y))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1, loss: 137.89727250142252\n",
            "iteration: 2, loss: 69.09063457346375\n",
            "iteration: 3, loss: 34.372926513882746\n",
            "iteration: 4, loss: 43.849807834622325\n",
            "iteration: 5, loss: 57.130106036466586\n",
            "iteration: 6, loss: 29.858920224417634\n",
            "iteration: 7, loss: 36.15970833723573\n",
            "iteration: 8, loss: 46.06527762294829\n",
            "iteration: 9, loss: 25.661219152411178\n",
            "iteration: 10, loss: 28.903151287994522\n",
            "iteration: 11, loss: 35.842611571837935\n",
            "iteration: 12, loss: 21.34826141224106\n",
            "iteration: 13, loss: 22.12422630416762\n",
            "iteration: 14, loss: 29.297385122671905\n",
            "iteration: 15, loss: 0.04364930155451743\n",
            "iteration: 16, loss: 0.0008920409560711436\n",
            "iteration: 17, loss: 0.0008178390161729273\n",
            "iteration: 18, loss: 0.0007553871811224882\n",
            "iteration: 19, loss: 0.0007020651028355893\n",
            "iteration: 20, loss: 0.0006559835415822302\n",
            "iteration: 21, loss: 0.0006157446659152544\n",
            "iteration: 22, loss: 0.0005802912241923469\n",
            "iteration: 23, loss: 0.0005488082659554359\n",
            "iteration: 24, loss: 0.0005206571622374424\n",
            "iteration: 25, loss: 0.0004953301484699658\n",
            "iteration: 26, loss: 0.0004724182890883413\n",
            "iteration: 27, loss: 0.0004515884448739598\n",
            "iteration: 28, loss: 0.00043256641573478243\n",
            "iteration: 29, loss: 0.0004151244049509016\n",
            "iteration: 30, loss: 0.00039907156216874957\n",
            "iteration: 31, loss: 0.00038424675555224624\n",
            "iteration: 32, loss: 0.00037051298179930823\n",
            "iteration: 33, loss: 0.00035775299577925823\n",
            "iteration: 34, loss: 0.0003458658595409913\n",
            "iteration: 35, loss: 0.0003347641921997562\n",
            "iteration: 36, loss: 0.00032437195971267305\n",
            "iteration: 37, loss: 0.0003146226845471293\n",
            "iteration: 38, loss: 0.00030545798483824354\n",
            "iteration: 39, loss: 0.0002968263742537324\n",
            "iteration: 40, loss: 0.0002886822697426745\n",
            "iteration: 41, loss: 0.00028098516625405814\n",
            "iteration: 42, loss: 0.0002736989464771277\n",
            "iteration: 43, loss: 0.00026679130046540337\n",
            "iteration: 44, loss: 0.0002602332352257597\n",
            "iteration: 45, loss: 0.00025399865838311356\n",
            "iteration: 46, loss: 0.00024806402316221286\n",
            "iteration: 47, loss: 0.00024240802438535067\n",
            "iteration: 48, loss: 0.00023701133711775743\n",
            "iteration: 49, loss: 0.00023185639112773647\n",
            "iteration: 50, loss: 0.00022692717555310933\n",
            "iteration: 51, loss: 0.00022220906915072888\n",
            "iteration: 52, loss: 0.00021768869229512653\n",
            "iteration: 53, loss: 0.0002133537775385787\n",
            "iteration: 54, loss: 0.0002091930560684753\n",
            "iteration: 55, loss: 0.00020519615782457176\n",
            "iteration: 56, loss: 0.00020135352339484584\n",
            "iteration: 57, loss: 0.00019765632609770905\n",
            "iteration: 58, loss: 0.00019409640289988887\n",
            "iteration: 59, loss: 0.0001906661930211198\n",
            "iteration: 60, loss: 0.00018735868324533056\n",
            "iteration: 61, loss: 0.00018416735909577764\n",
            "iteration: 62, loss: 0.00018108616115363547\n",
            "iteration: 63, loss: 0.00017810944589833594\n",
            "iteration: 64, loss: 0.00017523195052964485\n",
            "iteration: 65, loss: 0.00017244876130939515\n",
            "iteration: 66, loss: 0.0001697552850158207\n",
            "iteration: 67, loss: 0.00016714722315940112\n",
            "iteration: 68, loss: 0.00016462054865354114\n",
            "iteration: 69, loss: 0.00016217148467082386\n",
            "iteration: 70, loss: 0.00015979648544894648\n",
            "iteration: 71, loss: 0.00015749221884002243\n",
            "iteration: 72, loss: 0.0001552555504212034\n",
            "iteration: 73, loss: 0.00015308352900462216\n",
            "iteration: 74, loss: 0.0001509733734063457\n",
            "iteration: 75, loss: 0.00014892246034709923\n",
            "iteration: 76, loss: 0.00014692831337402402\n",
            "iteration: 77, loss: 0.00014498859270342115\n",
            "iteration: 78, loss: 0.00014310108589731319\n",
            "iteration: 79, loss: 0.00014126369929401989\n",
            "iteration: 80, loss: 0.00013947445012276174\n",
            "iteration: 81, loss: 0.0001377314592395863\n",
            "iteration: 82, loss: 0.00013603294442733754\n",
            "iteration: 83, loss: 0.00013437721421108786\n",
            "iteration: 84, loss: 0.00013276266213987597\n",
            "iteration: 85, loss: 0.00013118776149856538\n",
            "iteration: 86, loss: 0.0001296510604086144\n",
            "iteration: 87, loss: 0.00012815117728648222\n",
            "iteration: 88, loss: 0.00012668679663056968\n",
            "iteration: 89, loss: 0.00012525666510652825\n",
            "iteration: 90, loss: 0.00012385958790897832\n",
            "iteration: 91, loss: 0.00012249442537640672\n",
            "iteration: 92, loss: 0.00012116008983783377\n",
            "iteration: 93, loss: 0.00011985554267486067\n",
            "iteration: 94, loss: 0.0001185797915800104\n",
            "iteration: 95, loss: 0.0001173318879982218\n",
            "iteration: 96, loss: 0.00011611092473548908\n",
            "iteration: 97, loss: 0.00011491603372379045\n",
            "iteration: 98, loss: 0.00011374638392876629\n",
            "iteration: 99, loss: 0.00011260117939123852\n",
            "iteration: 100, loss: 0.00011147965739131445\n",
            "[array([9.99997836e-01, 1.71924699e-13, 2.16355663e-06]), array([5.46679880e-04, 9.99347951e-01, 1.05369310e-04]), array([3.06228281e-26, 7.39911344e-17, 1.00000000e+00]), array([9.99998220e-01, 1.21065049e-12, 1.78035280e-06]), array([4.20736956e-06, 9.99987328e-01, 8.46471337e-06]), array([3.94240981e-26, 1.96468059e-15, 1.00000000e+00])]\n",
            "0.00011147965739131445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5gWcuzhYpO6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}